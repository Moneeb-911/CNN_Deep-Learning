{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce0a939c-9e85-4f72-bb6d-9b494692f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import layers,models\n",
    "from PIL import Image\n",
    "# import torch\n",
    "# import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1efb245f-0658-4cdd-8dfc-1bfab0185deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2152 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE=(256,256)\n",
    "dataset=tf.keras.preprocessing.image_dataset_from_directory('C:\\\\Users\\\\ALVI COMPUTER\\\\Desktop\\\\Data_potatot_disease',\n",
    "                                                            shuffle=True,\n",
    "                                                            image_size=IMAGE_SIZE,\n",
    "                                                            batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96081f6c-9e21-472c-bf89-2037eac884bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fb18ff2-990e-4b32-a384-59bc8a9a0d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6df20d29-4c57-43c4-9333-0437900f623a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e114f1c-8c96-47cd-9195-cc070300e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_label=['Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "027e1a36-5d36-4c66-96bc-ce1ba1d476a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 0 1 0 2 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0 0 2 1 0]\n",
      "(32, 256, 256, 3)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\tensoflow\\lib\\site-packages\\PIL\\Image.py:3251\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3251\u001b[0m     \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   3252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\tensoflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:446\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    440\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    441\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;124m    If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;124m    from tensorflow.python.ops.numpy_ops import np_config\u001b[39m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;124m    np_config.enable_numpy_behavior()\u001b[39m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m--> 446\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m image_np \u001b[38;5;241m=\u001b[39m image_batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m img_path \u001b[38;5;241m=\u001b[39m image_np \n\u001b[1;32m----> 6\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m img\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\tensoflow\\lib\\site-packages\\PIL\\Image.py:3253\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3251\u001b[0m     fp\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   3252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation):\n\u001b[1;32m-> 3253\u001b[0m     fp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m())\n\u001b[0;32m   3254\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3256\u001b[0m prefix \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m16\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\tensoflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:446\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mravel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranspose\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    438\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtolist\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m    439\u001b[0m   \u001b[38;5;66;03m# TODO(wangpeng): Export the enable_numpy_behavior knob\u001b[39;00m\n\u001b[0;32m    440\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    441\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;124m    If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;124m    from tensorflow.python.ops.numpy_ops import np_config\u001b[39m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;124m    np_config.enable_numpy_behavior()\u001b[39m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m--> 446\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "for image_batch,label in dataset.take(1):\n",
    "    print(label.numpy())\n",
    "    print(image_batch.shape)\n",
    "    image_np = image_batch[0]\n",
    "    img_path = image_np \n",
    "    img = Image.open(img_path)\n",
    "    img.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce46e9a-59b2-406c-a83d-0605e4476bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(data,shuffle=True,shuffle_size=10000):\n",
    "    if shuffle:\n",
    "        data=data.shuffle(shuffle_size,seed=12)\n",
    "        train_size=0.8*len(data)\n",
    "        train_data=data.take(int(train_size))\n",
    "        rem_data=data.skip(int(train_size))\n",
    "        test_size=len(data)*0.1\n",
    "        test_data=rem_data.take(int(test_size))\n",
    "        val_data=rem_data.skip(int(test_size))\n",
    "        return train_data,test_data,val_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34603fb7-f6f9-4109-8c74-5b36c6af8a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data,val_data=partition(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "432c1f6b-312e-4b92-b0c5-a605ecdcbfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a208f99-35bc-4844-aa51-68831af65362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dc4515f-0507-4721-89a2-22a8a325e690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de02687d-1e54-4353-b258-7293c3ca9815",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_data=test_data.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_data=val_data.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "411826cf-4f20-41f3-b71c-2d29ecd91529",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_rescale_layer=tf.keras.Sequential(layers.experimental.preprocessing.Resizing(256,256),\n",
    "                                          layers.experimental.preprocessing.Rescaling(1.0/255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f22896d9-2f6d-4940-9b1a-157e8a13fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation_layer=tf.keras.Sequential(layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "                                            layers.experimental.preprocessing.RandomRotation(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5af1891-422c-41ed-a468-9124c03a0d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels=3\n",
    "batch_size=32\n",
    "image_size=256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e64da414-44d2-45a0-9148-88e7f280e325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "input_shape=(image_size,image_size,channels)\n",
    "model = models.Sequential([\n",
    "    layers.InputLayer(input_shape=input_shape),\n",
    "    layers.experimental.preprocessing.Resizing(256,256),\n",
    "    layers.experimental.preprocessing.Rescaling(1.0/255),\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "    layers.Conv2D(32, (3, 3), strides=(1, 1), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), strides=(1, 1), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), strides=(1, 1), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), strides=(1, 1), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef9ddb29-ce7c-410f-9825-3086161a809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cdefeb4-82bf-45de-9076-533e45883b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "54/54 [==============================] - 117s 2s/step - loss: 0.6550 - accuracy: 0.7135 - val_loss: 0.3757 - val_accuracy: 0.8750\n",
      "Epoch 2/30\n",
      "54/54 [==============================] - 100s 2s/step - loss: 0.2126 - accuracy: 0.9144 - val_loss: 0.2582 - val_accuracy: 0.8750\n",
      "Epoch 3/30\n",
      "54/54 [==============================] - 92s 2s/step - loss: 0.2657 - accuracy: 0.9010 - val_loss: 0.1693 - val_accuracy: 0.9336\n",
      "Epoch 4/30\n",
      "54/54 [==============================] - 91s 2s/step - loss: 0.1521 - accuracy: 0.9427 - val_loss: 0.0951 - val_accuracy: 0.9727\n",
      "Epoch 5/30\n",
      "54/54 [==============================] - 90s 2s/step - loss: 0.1284 - accuracy: 0.9520 - val_loss: 0.2793 - val_accuracy: 0.8633\n",
      "Epoch 6/30\n",
      "54/54 [==============================] - 91s 2s/step - loss: 0.1319 - accuracy: 0.9514 - val_loss: 0.5199 - val_accuracy: 0.7773\n",
      "Epoch 7/30\n",
      "54/54 [==============================] - 90s 2s/step - loss: 0.1161 - accuracy: 0.9583 - val_loss: 0.4332 - val_accuracy: 0.8398\n",
      "Epoch 8/30\n",
      "54/54 [==============================] - 91s 2s/step - loss: 0.1259 - accuracy: 0.9520 - val_loss: 0.1121 - val_accuracy: 0.9648\n",
      "Epoch 9/30\n",
      "54/54 [==============================] - 91s 2s/step - loss: 0.0734 - accuracy: 0.9745 - val_loss: 0.2252 - val_accuracy: 0.9062\n",
      "Epoch 10/30\n",
      "54/54 [==============================] - 88s 2s/step - loss: 0.0870 - accuracy: 0.9722 - val_loss: 0.0851 - val_accuracy: 0.9609\n",
      "Epoch 11/30\n",
      "54/54 [==============================] - 88s 2s/step - loss: 0.1223 - accuracy: 0.9508 - val_loss: 0.1251 - val_accuracy: 0.9648\n",
      "Epoch 12/30\n",
      "54/54 [==============================] - 89s 2s/step - loss: 0.0756 - accuracy: 0.9728 - val_loss: 0.2334 - val_accuracy: 0.8906\n",
      "Epoch 13/30\n",
      "54/54 [==============================] - 89s 2s/step - loss: 0.0845 - accuracy: 0.9676 - val_loss: 0.0607 - val_accuracy: 0.9844\n",
      "Epoch 14/30\n",
      "54/54 [==============================] - 89s 2s/step - loss: 0.0746 - accuracy: 0.9751 - val_loss: 0.0671 - val_accuracy: 0.9844\n",
      "Epoch 15/30\n",
      "54/54 [==============================] - 89s 2s/step - loss: 0.0907 - accuracy: 0.9670 - val_loss: 0.0682 - val_accuracy: 0.9883\n",
      "Epoch 16/30\n",
      "54/54 [==============================] - 89s 2s/step - loss: 0.0969 - accuracy: 0.9659 - val_loss: 0.2825 - val_accuracy: 0.9180\n",
      "Epoch 17/30\n",
      "54/54 [==============================] - 88s 2s/step - loss: 0.0666 - accuracy: 0.9745 - val_loss: 0.3372 - val_accuracy: 0.8555\n",
      "Epoch 18/30\n",
      "54/54 [==============================] - 90s 2s/step - loss: 0.0963 - accuracy: 0.9635 - val_loss: 0.1032 - val_accuracy: 0.9688\n",
      "Epoch 19/30\n",
      "54/54 [==============================] - 90s 2s/step - loss: 0.0663 - accuracy: 0.9780 - val_loss: 0.0575 - val_accuracy: 0.9805\n",
      "Epoch 20/30\n",
      "54/54 [==============================] - 89s 2s/step - loss: 0.0841 - accuracy: 0.9705 - val_loss: 0.1858 - val_accuracy: 0.9414\n",
      "Epoch 21/30\n",
      "54/54 [==============================] - 88s 2s/step - loss: 0.0795 - accuracy: 0.9682 - val_loss: 0.0837 - val_accuracy: 0.9766\n",
      "Epoch 22/30\n",
      "54/54 [==============================] - 87s 2s/step - loss: 0.0667 - accuracy: 0.9774 - val_loss: 0.1253 - val_accuracy: 0.9531\n",
      "Epoch 23/30\n",
      "54/54 [==============================] - 88s 2s/step - loss: 0.0757 - accuracy: 0.9763 - val_loss: 0.0910 - val_accuracy: 0.9570\n",
      "Epoch 24/30\n",
      "54/54 [==============================] - 106s 2s/step - loss: 0.0447 - accuracy: 0.9826 - val_loss: 0.0548 - val_accuracy: 0.9883\n",
      "Epoch 25/30\n",
      "54/54 [==============================] - 100s 2s/step - loss: 0.0433 - accuracy: 0.9838 - val_loss: 0.0551 - val_accuracy: 0.9805\n",
      "Epoch 26/30\n",
      "54/54 [==============================] - 88s 2s/step - loss: 0.0580 - accuracy: 0.9780 - val_loss: 0.1217 - val_accuracy: 0.9492\n",
      "Epoch 27/30\n",
      "54/54 [==============================] - 88s 2s/step - loss: 0.0437 - accuracy: 0.9850 - val_loss: 0.1534 - val_accuracy: 0.9258\n",
      "Epoch 28/30\n",
      "54/54 [==============================] - 88s 2s/step - loss: 0.0391 - accuracy: 0.9873 - val_loss: 0.0515 - val_accuracy: 0.9883\n",
      "Epoch 29/30\n",
      "54/54 [==============================] - 87s 2s/step - loss: 0.1553 - accuracy: 0.9450 - val_loss: 0.1282 - val_accuracy: 0.9336\n",
      "Epoch 30/30\n",
      "54/54 [==============================] - 88s 2s/step - loss: 0.0675 - accuracy: 0.9745 - val_loss: 0.1503 - val_accuracy: 0.9414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dd30d41730>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data,\n",
    "          epochs=30,\n",
    "          batch_size=32,\n",
    "          verbose=1,\n",
    "          validation_data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09cc2f8b-a50c-4d4f-a394-ba4859dee76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resizing_1 (Resizing)       (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " rescaling_1 (Rescaling)     (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " random_flip_1 (RandomFlip)  (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " random_rotation_1 (RandomRo  (None, 256, 256, 3)      0         \n",
      " tation)                                                         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 127, 127, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 62, 62, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 60, 60, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 30, 30, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12544)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                802880    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 896,323\n",
      "Trainable params: 896,323\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30063457-f49f-46d4-a6b9-882b2e5ef325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 4s 331ms/step - loss: 0.3149 - accuracy: 0.8958\n"
     ]
    }
   ],
   "source": [
    "res=model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c3ca3ab-d5a0-4f91-b363-5a6d789f2af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ALVI COMPUTER\\Desktop\\CNN_Potato_Disease\\1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ALVI COMPUTER\\Desktop\\CNN_Potato_Disease\\1\\assets\n"
     ]
    }
   ],
   "source": [
    "model_version=1\n",
    "model.save(f\"C:\\\\Users\\\\ALVI COMPUTER\\\\Desktop\\\\CNN_Potato_Disease\\\\{model_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eced609f-8413-4375-a275-ab98873b504f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "new_model=tf.keras.models.load_model('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ada388b0-e862-4d93-b26f-1178755ecd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resizing_1 (Resizing)       (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " rescaling_1 (Rescaling)     (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " random_flip_1 (RandomFlip)  (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " random_rotation_1 (RandomRo  (None, 256, 256, 3)      0         \n",
      " tation)                                                         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 127, 127, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 62, 62, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 60, 60, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 30, 30, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12544)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                802880    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 896,323\n",
      "Trainable params: 896,323\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39a99fb8-252d-4463-88d8-2e5a0f1e04dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 296ms/step - loss: 0.3149 - accuracy: 0.8958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.31493842601776123, 0.8958333134651184]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.evaluate(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
